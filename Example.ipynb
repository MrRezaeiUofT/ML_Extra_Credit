{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Architecture for Decoding EEG MI Data using Spectrogram Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "In case that gumpy is not installed as a module, we need to specify the path to ``gumpy``. In addition, we wish to configure jupyter notebooks and any backend properly. Note that it may take some time for ``gumpy`` to load due to the number of dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os; os.environ[\"THEANO_FLAGS\"] = \"device=gpu0\"\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.append('./gumpy')\n",
    "\n",
    "import gumpy\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To use the models provided by `gumpy-deeplearning`, we have to set the path to the models directory and import it. If you installed `gumpy-deeplearning` as a module, this step may not be required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "\n",
    "The examples for ``gumpy-deeplearning`` ship with a few tiny helper functions. For instance, there's one that tells you the versions of the currently installed keras and kapre. ``keras`` is required in ``gumpy-deeplearning``, while ``kapre`` \n",
    "can be used to compute spectrograms.\n",
    "\n",
    "In addition, the utility functions contain a method ``load_preprocess_data`` to load and preprocess data. Its usage will be shown further below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/panagiotisargyrakis/.conda/envs/Project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/panagiotisargyrakis/.conda/envs/Project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/panagiotisargyrakis/.conda/envs/Project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/panagiotisargyrakis/.conda/envs/Project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/panagiotisargyrakis/.conda/envs/Project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/panagiotisargyrakis/.conda/envs/Project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "2020/2/12\n",
      "Keras version: 2.2.4\n",
      "Keras backend: tensorflow: 1.13.1\n",
      "Keras image dim ordering: tf\n",
      "Kapre version: 0.1.4\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import utils\n",
    "utils.print_version_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup parameters for the model and data\n",
    "Before we jump into the processing, we first wish to specify some parameters (e.g. frequencies) that we know from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "CLASS_COUNT = 2\n",
    "DROPOUT = 0.2   # dropout rate in float\n",
    "\n",
    "# parameters for filtering data\n",
    "FS = 250\n",
    "LOWCUT = 2\n",
    "HIGHCUT = 60\n",
    "ANTI_DRIFT = 0.5\n",
    "CUTOFF = 50.0 # freq to be removed from signal (Hz) for notch filter\n",
    "Q = 30.0  # quality factor for notch filter \n",
    "W0 = CUTOFF/(FS/2)\n",
    "AXIS = 0\n",
    "\n",
    "#set random seed\n",
    "SEED = 42\n",
    "KFOLD = 5\n",
    "NumbItr=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data\n",
    "\n",
    "Before training and testing a model, we need some data. The following code shows how to load a dataset using ``gumpy``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Band-pass filtering the data in frequency range from 2.0 Hz to 60.0 Hz... \n",
      "Data loaded and processed successfully!\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# specify the location of the GrazB datasets\n",
    "data_dir = './data/Graz'\n",
    "subject = 'B01'\n",
    "\n",
    "# initialize the data-structure, but do _not_ load the data yet\n",
    "grazb_data = gumpy.data.GrazB(data_dir, subject)\n",
    "\n",
    "# now that the dataset is setup, we can load the data. This will be handled from within the utils function, \n",
    "# which will first load the data and subsequently filter it using a notch and a bandpass filter.\n",
    "# the utility function will then return the training data.\n",
    "x_train, y_train = utils.load_preprocess_data(grazb_data, True, LOWCUT, HIGHCUT, W0, Q, ANTI_DRIFT, CLASS_COUNT, CUTOFF, AXIS, FS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "x_augmented, y_augmented = gumpy.signal.sliding_window(data = x_train[:,:,:],\n",
    "                                                          labels = y_train[:,:],\n",
    "                                                          window_sz = 4 * FS,\n",
    "                                                          n_hop = FS // 10,\n",
    "                                                          n_start = FS * 1)\n",
    "x_subject = x_augmented\n",
    "y_subject = y_augmented\n",
    "x_subject = np.rollaxis(x_subject, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#from .model import KerasModel\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import BatchNormalization, Dropout, Conv2D, MaxPooling2D\n",
    "from keras.models import load_model\n",
    "import kapre\n",
    "from kapre.utils import Normalization2D\n",
    "from kapre.time_frequency import Spectrogram\n",
    "\n",
    "def CNN_model(input_shape, dropout=0.5, print_summary=False):\n",
    "\n",
    "        # basis of the CNN_STFT is a Sequential network\n",
    "        model = Sequential()\n",
    "\n",
    "        # spectrogram creation using STFT\n",
    "        model.add(Spectrogram(n_dft = 128, n_hop = 16, input_shape = input_shape,\n",
    "                  return_decibel_spectrogram = False, power_spectrogram = 2.0,\n",
    "                  trainable_kernel = False, name = 'static_stft'))\n",
    "        model.add(Normalization2D(str_axis = 'freq'))\n",
    "\n",
    "        # Conv Block 1\n",
    "        model.add(Conv2D(filters = 24, kernel_size = (12, 12),\n",
    "                         strides = (1, 1), name = 'conv1',\n",
    "                         border_mode = 'same'))\n",
    "        model.add(BatchNormalization(axis = 1))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2), strides = (2,2), padding = 'valid',\n",
    "                               data_format = 'channels_last'))\n",
    "\n",
    "        # Conv Block 2\n",
    "        model.add(Conv2D(filters = 48, kernel_size = (8, 8),\n",
    "                         name = 'conv2', border_mode = 'same'))\n",
    "        model.add(BatchNormalization(axis = 1))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'valid',\n",
    "                               data_format = 'channels_last'))\n",
    "\n",
    "        # Conv Block 3\n",
    "        model.add(Conv2D(filters = 96, kernel_size = (4, 4),\n",
    "                         name = 'conv3', border_mode = 'same'))\n",
    "        model.add(BatchNormalization(axis = 1))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2), strides = (2,2),\n",
    "                               padding = 'valid',\n",
    "                               data_format = 'channels_last'))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # classificator\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(2))  # two classes only\n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "        if print_summary:\n",
    "            print(model.summary())\n",
    "\n",
    "        # compile the model\n",
    "        model.compile(loss = 'categorical_crossentropy',\n",
    "                      optimizer = 'adam',\n",
    "                      metrics = ['accuracy'])\n",
    "\n",
    "        # assign model and return\n",
    "        \n",
    "        \n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Run 1...\n",
      "WARNING:tensorflow:From /Users/panagiotisargyrakis/.conda/envs/Project/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/panagiotisargyrakis/.conda/envs/Project/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/panagiotisargyrakis/.conda/envs/Project/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.04478, saving model to GRAZ_CNN_STFT_3layer__run_1\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "/Users/panagiotisargyrakis/.conda/envs/Project/lib/python3.7/site-packages/ipykernel_launcher.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(filters=24, kernel_size=(12, 12), strides=(1, 1), name=\"conv1\", padding=\"same\")`\n",
      "/Users/panagiotisargyrakis/.conda/envs/Project/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(filters=48, kernel_size=(8, 8), name=\"conv2\", padding=\"same\")`\n",
      "/Users/panagiotisargyrakis/.conda/envs/Project/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(filters=96, kernel_size=(4, 4), name=\"conv3\", padding=\"same\")`\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# define KFOLD-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits = KFOLD, shuffle = True, random_state = SEED)\n",
    "cvscores = []\n",
    "ii = 1\n",
    "for train, test in kfold.split(x_subject, y_subject[:, 0]):\n",
    "    print('Run ' + str(ii) + '...')\n",
    "    # create callbacks\n",
    "    model_name_str = 'GRAZ_CNN_STFT_3layer_' + \\\n",
    "                     '_run_' + str(ii)\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(model_name_str, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    # Fit the model\n",
    "    # initialize and create the model\n",
    "    model = CNN_model(x_subject.shape[1:], dropout = DROPOUT, print_summary = False)\n",
    "    \n",
    "    # fit model. If you specify monitor=True, then the model will create callbacks\n",
    "    # and write its state to a HDF5 file\n",
    "    model.fit(x_subject[train], y_subject[train],\n",
    "              epochs = NumbItr, \n",
    "              batch_size = 256, \n",
    "              verbose = 0, \n",
    "              validation_split = 0.1, callbacks = callbacks_list)\n",
    "\n",
    "    # evaluate the model\n",
    "    print('Evaluating model on test set...')\n",
    "    scores = model.evaluate(x_subject[test], y_subject[test], verbose = 0)\n",
    "    print(\"Result on test set: %s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    ii += 1\n",
    "    \n",
    "# print some evaluation statistics and write results to file\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "cv_all_subjects = np.asarray(cvscores)\n",
    "print('Saving CV values to file....')\n",
    "np.savetxt('GRAZ_CV_' + 'CNN_STFT_3layer_' + str(DROPOUT) + 'do'+'.csv', \n",
    "            cv_all_subjects, delimiter = ',', fmt = '%2.4f')\n",
    "print('CV values successfully saved!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model.save('CNN_STFTmonitoring.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "model2 = load_model('CNN_STFTmonitoring.h5', \n",
    "                                 custom_objects={'Spectrogram': kapre.time_frequency.Spectrogram, \n",
    "                                                 'Normalization2D': kapre.utils.Normalization2D})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN, Dense, LSTM as _LSTM\n",
    "def LSTM_model(input_shape, num_hidden_neurons=128,\n",
    "                      num_layers=1, dropout=0.2, recurrent_dropout=0.2,\n",
    "                      print_summary=False):\n",
    "    model = Sequential()\n",
    "    if num_layers > 1:\n",
    "        for i in range(1, num_layers, 1):\n",
    "            model.add(_LSTM(num_hidden_neurons, input_shape=input_shape,\n",
    "                            return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "        model.add(_LSTM(num_hidden_neurons))\n",
    "    else:\n",
    "        model.add(_LSTM(num_hidden_neurons, input_shape=input_shape, dropout=dropout,\n",
    "                        recurrent_dropout=recurrent_dropout))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    if print_summary:\n",
    "        print(model.summary())\n",
    "\n",
    "        # compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer='adam',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "        # assign and return\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# define KFOLD-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits = KFOLD, shuffle = True, random_state = SEED)\n",
    "cvscores = []\n",
    "ii = 1\n",
    "for train, test in kfold.split(x_subject, y_subject[:, 0]):\n",
    "    print('Run ' + str(ii) + '...')\n",
    "    # create callbacks\n",
    "    model_name_str = 'GRAZ_LSTM_' + \\\n",
    "                     '_run_' + str(ii)\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(model_name_str, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    # Fit the model\n",
    "    # initialize and create the model\n",
    "    model = LSTM_model(x_subject.shape[1:], dropout = DROPOUT, print_summary = False)\n",
    "    \n",
    "    # fit model. If you specify monitor=True, then the model will create callbacks\n",
    "    # and write its state to a HDF5 file\n",
    "    model.fit(x_subject[train], y_subject[train],\n",
    "              epochs = NumbItr, \n",
    "              batch_size = 256, \n",
    "              verbose = 1, \n",
    "              validation_split = 0.1, callbacks = callbacks_list)\n",
    "\n",
    "    # evaluate the model\n",
    "    print('Evaluating model on test set...')\n",
    "    scores = model.evaluate(x_subject[test], y_subject[test], verbose = 0)\n",
    "    print(\"Result on test set: %s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    ii += 1\n",
    "    \n",
    "# print some evaluation statistics and write results to file\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "cv_all_subjects = np.asarray(cvscores)\n",
    "print('Saving CV values to file....')\n",
    "np.savetxt('GRAZ_CV_' + 'LSTM_' + str(DROPOUT) + 'do'+'.csv', \n",
    "            cv_all_subjects, delimiter = ',', fmt = '%2.4f')\n",
    "print('CV values successfully saved!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model.save('LSTMmonitoring.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "model2 = load_model('LSTMmonitoring.h5', \n",
    "                                 custom_objects={'Spectrogram': kapre.time_frequency.Spectrogram, \n",
    "                                                 'Normalization2D': kapre.utils.Normalization2D})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#from .model import KerasModel\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import BatchNormalization, Dropout, Conv2D, MaxPooling2D\n",
    "\n",
    "import kapre\n",
    "from kapre.utils import Normalization2D\n",
    "from kapre.time_frequency import Spectrogram\n",
    "\n",
    "def MLP_model(input_shape, dropout=0.5, print_summary=False):\n",
    "\n",
    "        # basis of the CNN_STFT is a Sequential network\n",
    "        model = Sequential()\n",
    "\n",
    "        # spectrogram creation using STFT\n",
    "        model.add(Spectrogram(n_dft = 128, n_hop = 16, input_shape = input_shape,\n",
    "                  return_decibel_spectrogram = False, power_spectrogram = 2.0,\n",
    "                  trainable_kernel = False, name = 'static_stft'))\n",
    "        model.add(Normalization2D(str_axis = 'freq'))\n",
    "        model.add(Flatten())  \n",
    "        model.add(Dense(100, activation='relu', input_shape=(784,)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(100, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(2))  # two classes only\n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "        if print_summary:\n",
    "            print(model.summary())\n",
    "\n",
    "        # compile the model\n",
    "        model.compile(loss = 'categorical_crossentropy',\n",
    "                      optimizer = 'adam',\n",
    "                      metrics = ['accuracy'])\n",
    "\n",
    "        # assign model and return\n",
    "        \n",
    "        \n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# define KFOLD-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits = KFOLD, shuffle = True, random_state = SEED)\n",
    "cvscores = []\n",
    "ii = 1\n",
    "for train, test in kfold.split(x_subject, y_subject[:, 0]):\n",
    "    print('Run ' + str(ii) + '...')\n",
    "    # create callbacks\n",
    "    model_name_str = 'GRAZ_MLP_' + \\\n",
    "                     '_run_' + str(ii)\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(model_name_str, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    # Fit the model\n",
    "    # initialize and create the model\n",
    "    model = MLP_model(x_subject.shape[1:], dropout = DROPOUT, print_summary = False)\n",
    "    \n",
    "    # fit model. If you specify monitor=True, then the model will create callbacks\n",
    "    # and write its state to a HDF5 file\n",
    "    model.fit(x_subject[train], y_subject[train],\n",
    "              epochs = NumbItr, \n",
    "              batch_size = 256, \n",
    "              verbose = 1, \n",
    "              validation_split = 0.1, callbacks = callbacks_list)\n",
    "\n",
    "    # evaluate the model\n",
    "    print('Evaluating model on test set...')\n",
    "    scores = model.evaluate(x_subject[test], y_subject[test], verbose = 0)\n",
    "    print(\"Result on test set: %s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    ii += 1\n",
    "    \n",
    "# print some evaluation statistics and write results to file\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "cv_all_subjects = np.asarray(cvscores)\n",
    "print('Saving CV values to file....')\n",
    "np.savetxt('GRAZ_CV_' + 'MLP_' + str(DROPOUT) + 'do'+'.csv', \n",
    "            cv_all_subjects, delimiter = ',', fmt = '%2.4f')\n",
    "print('CV values successfully saved!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model.save('MLPmonitoring.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "model2 = load_model('MLPmonitoring.h5', \n",
    "                                 custom_objects={'Spectrogram': kapre.time_frequency.Spectrogram, \n",
    "                                                 'Normalization2D': kapre.utils.Normalization2D})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}